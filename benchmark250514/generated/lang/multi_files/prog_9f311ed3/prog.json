{
    "files": {
        "TokenProcessor.java": "import org.apache.commons.lang3.text.StrTokenizer;\nimport java.util.ArrayList;\nimport java.util.HashMap;\n\npublic class TokenProcessor {\n    private StrTokenizer tokenizer;\n    private ArrayList<String> tokenList = new ArrayList<>();\n    private HashMap<String, Integer> tokenCountMap = new HashMap<>();\n\n    public TokenProcessor(String input, char delimiter) {\n        tokenizer = new StrTokenizer(input, delimiter);\n        /* write */ tokenizer.setEmptyTokenAsNull(true);\n    }\n\n    public void processTokens() {\n        while (tokenizer.hasNext()) {\n            String token = tokenizer.nextToken();\n            if (token != null) {\n                tokenList.add(token);\n                tokenCountMap.put(token, tokenCountMap.getOrDefault(token, 0) + 1);\n            }\n        }\n    }\n\n    public ArrayList<String> getTokenList() {\n        return tokenList;\n    }\n\n    public HashMap<String, Integer> getTokenCountMap() {\n        return tokenCountMap;\n    }\n}\n",
        "TokenAnalyzer.java": "import java.util.ArrayList;\nimport java.util.HashMap;\n\npublic class TokenAnalyzer {\n    private TokenProcessor processor;\n\n    public TokenAnalyzer(TokenProcessor processor) {\n        this.processor = processor;\n    }\n\n    public void displayTokenCounts() {\n        HashMap<String, Integer> tokenCountMap = processor.getTokenCountMap();\n        for (String key : tokenCountMap.keySet()) {\n            System.out.println(\"Token: \" + key + \", Count: \" + tokenCountMap.get(key));\n        }\n    }\n\n    public void displayTokens() {\n        ArrayList<String> tokenList = processor.getTokenList();\n        for (int i = 0; i < tokenList.size(); i++) {\n            System.out.println(\"Token from list: \" + tokenList.get(i));\n        }\n    }\n}\n",
        "Main.java": "public class Main {\n    public static void main(String[] args) {\n        TokenProcessor processor = new TokenProcessor(\"a,b,c\", ',');\n        processor.processTokens();\n\n        TokenAnalyzer analyzer = new TokenAnalyzer(processor);\n        analyzer.displayTokenCounts();\n        analyzer.displayTokens();\n\n        // Reset tokenizer and process again to demonstrate read\n        processor = new TokenProcessor(\"a,b,c\", ',');\n        /* read */ String firstToken = processor.getTokenList().get(0);\n        System.out.println(\"First token: \" + firstToken);\n    }\n}"
    },
    "pair": {
        "write_class": "org.apache.commons.lang3.text.StrTokenizer",
        "write_method": "setEmptyTokenAsNull",
        "read_class": "org.apache.commons.lang3.text.StrTokenizer",
        "read_method": "nextToken"
    },
    "java_code_simple": "import org.apache.commons.lang3.text.StrTokenizer;\n\npublic class Main {\n    public static void main(String[] args) {\n        StrTokenizer tokenizer = new StrTokenizer(\"a,b,c\", ',');\n        /* write */ tokenizer.setEmptyTokenAsNull(true);\n        /* read */ String token = tokenizer.nextToken();\n        System.out.println(token);\n    }\n}",
    "java_code_complex": "import org.apache.commons.lang3.text.StrTokenizer;\nimport java.util.HashMap;\nimport java.util.ArrayList;\n\npublic class Main {\n    public static void main(String[] args) {\n        // Initialize a StrTokenizer with a simple string\n        StrTokenizer tokenizer = new StrTokenizer(\"a,b,c\", ',');\n\n        // Create a HashMap to store tokens and their counts\n        HashMap<String, Integer> tokenCountMap = new HashMap<>();\n\n        // Create an ArrayList to hold tokens temporarily\n        ArrayList<String> tokenList = new ArrayList<>();\n\n        // Configure tokenizer to treat empty tokens as null\n        /* write */ tokenizer.setEmptyTokenAsNull(true);\n\n        // Populate the ArrayList with tokens\n        while (tokenizer.hasNext()) {\n            String token = tokenizer.nextToken();\n            if (token != null) {\n                tokenList.add(token);\n            }\n        }\n\n        // Count occurrences of each token\n        for (String token : tokenList) {\n            tokenCountMap.put(token, tokenCountMap.getOrDefault(token, 0) + 1);\n        }\n\n        // Print token counts\n        for (String key : tokenCountMap.keySet()) {\n            System.out.println(\"Token: \" + key + \", Count: \" + tokenCountMap.get(key));\n        }\n\n        // Reset tokenizer to process the same string again\n        tokenizer.reset();\n\n        // Process tokens from the tokenizer and print them\n        /* read */ String token = tokenizer.nextToken();\n        System.out.println(\"First token: \" + token);\n\n        // Further processing of tokens\n        while (tokenizer.hasNext()) {\n            String nextToken = tokenizer.nextToken();\n            System.out.println(\"Next token: \" + nextToken);\n        }\n\n        // Additional complex operations\n        for (int i = 0; i < tokenList.size(); i++) {\n            System.out.println(\"Token from list: \" + tokenList.get(i));\n        }\n    }\n}"
}