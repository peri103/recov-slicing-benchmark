{
    "files": {
        "TokenProcessor.java": "import org.apache.commons.lang3.text.StrTokenizer;\nimport java.util.ArrayList;\n\npublic class TokenProcessor {\n    private StrTokenizer tokenizer;\n    private ArrayList<String> tokenList = new ArrayList<>();\n\n    public TokenProcessor(String input) {\n        tokenizer = new StrTokenizer(input);\n    }\n\n    public void setDelimiter(String delimiter) {\n        /* write */ tokenizer.setDelimiterString(delimiter);\n    }\n\n    public void processTokens() {\n        while (tokenizer.hasNext()) {\n            tokenList.add(tokenizer.nextToken());\n        }\n    }\n\n    public ArrayList<String> getTokenList() {\n        return tokenList;\n    }\n}\n",
        "TokenAnalyzer.java": "import java.util.HashMap;\nimport java.util.List;\n\npublic class TokenAnalyzer {\n    private HashMap<String, Integer> tokenCountMap = new HashMap<>();\n\n    public void analyzeTokens(List<String> tokens) {\n        for (String token : tokens) {\n            tokenCountMap.put(token, tokenCountMap.getOrDefault(token, 0) + 1);\n        }\n    }\n\n    public HashMap<String, Integer> getTokenCountMap() {\n        return tokenCountMap;\n    }\n\n    public String getFirstToken(List<String> tokens) {\n        /* read */ return tokens.get(0);\n    }\n}\n",
        "Main.java": "import java.util.ArrayList;\n\npublic class Main {\n    public static void main(String[] args) {\n        // Initialize TokenProcessor with a sample input\n        TokenProcessor processor = new TokenProcessor(\"Hello,World,Java\");\n\n        // Set the delimiter and process tokens\n        processor.setDelimiter(\",\");\n        processor.processTokens();\n\n        // Retrieve processed tokens\n        ArrayList<String> tokens = processor.getTokenList();\n\n        // Initialize TokenAnalyzer and analyze tokens\n        TokenAnalyzer analyzer = new TokenAnalyzer();\n        analyzer.analyzeTokens(tokens);\n\n        // Retrieve the first token\n        String firstToken = analyzer.getFirstToken(tokens);\n\n        // Print the first token\n        System.out.println(\"First Token: \" + firstToken);\n\n        // Print all tokens and their counts\n        System.out.println(\"All Tokens and Counts:\");\n        for (String token : analyzer.getTokenCountMap().keySet()) {\n            System.out.println(\"Token: \" + token + \", Count: \" + analyzer.getTokenCountMap().get(token));\n        }\n    }\n}"
    },
    "pair": {
        "write_class": "org.apache.commons.lang3.text.StrTokenizer",
        "write_method": "setDelimiterString",
        "read_class": "org.apache.commons.lang3.text.StrTokenizer",
        "read_method": "nextToken"
    },
    "java_code_simple": "import org.apache.commons.lang3.text.StrTokenizer;\n\npublic class Main {\n    public static void main(String[] args) {\n        // Initialize the StrTokenizer with a sample string\n        StrTokenizer tokenizer = new StrTokenizer(\"Hello,World,Java\");\n\n        // Set the delimiter string to a comma\n        /* write */ tokenizer.setDelimiterString(\",\");\n\n        // Read the first token\n        /* read */ String token = tokenizer.nextToken();\n\n        // Print the token\n        System.out.println(token);\n    }\n}",
    "java_code_complex": "import org.apache.commons.lang3.text.StrTokenizer;\nimport java.util.ArrayList;\nimport java.util.HashMap;\n\npublic class Main {\n    public static void main(String[] args) {\n        // Initialize the StrTokenizer with a sample string\n        StrTokenizer tokenizer = new StrTokenizer(\"Hello,World,Java\");\n\n        // Create an ArrayList to store tokens\n        ArrayList<String> tokenList = new ArrayList<>();\n\n        // Create a HashMap to count occurrences of each token\n        HashMap<String, Integer> tokenCountMap = new HashMap<>();\n\n        // Set the delimiter string to a comma\n        /* write */ tokenizer.setDelimiterString(\",\");\n\n        // Process tokens and store them in the ArrayList\n        while (tokenizer.hasNext()) {\n            String currentToken = tokenizer.nextToken();\n            tokenList.add(currentToken);\n        }\n\n        // Count occurrences of each token\n        for (String token : tokenList) {\n            tokenCountMap.put(token, tokenCountMap.getOrDefault(token, 0) + 1);\n        }\n\n        // Read the first token\n        /* read */ String firstToken = tokenList.get(0);\n\n        // Print the first token\n        System.out.println(\"First Token: \" + firstToken);\n\n        // Print all tokens and their counts\n        System.out.println(\"All Tokens and Counts:\");\n        for (String token : tokenCountMap.keySet()) {\n            System.out.println(\"Token: \" + token + \", Count: \" + tokenCountMap.get(token));\n        }\n\n        // Perform some additional operations\n        int sumOfLengths = 0;\n        for (String token : tokenList) {\n            sumOfLengths += token.length();\n        }\n        System.out.println(\"Sum of token lengths: \" + sumOfLengths);\n    }\n}"
}